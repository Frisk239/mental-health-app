"""
é¢éƒ¨è¡¨æƒ…è¯†åˆ«æœåŠ¡
ä½¿ç”¨OpenCVè¿›è¡Œé¢éƒ¨æ£€æµ‹ï¼ŒViTæ¨¡å‹è¿›è¡Œè¡¨æƒ…åˆ†ç±»
"""

import cv2
import numpy as np
from PIL import Image
import torch
from transformers import ViTImageProcessor, ViTForImageClassification
import logging
import os
from typing import Dict, List, Tuple, Optional
import json
import random

logger = logging.getLogger(__name__)

class EmotionRecognitionService:
    """
    é¢éƒ¨è¡¨æƒ…è¯†åˆ«æœåŠ¡ç±»
    æ•´åˆOpenCVé¢éƒ¨æ£€æµ‹å’ŒViTæ·±åº¦å­¦ä¹ æ¨¡å‹
    """

    def __init__(self, model_path: str = None):
        """
        åˆå§‹åŒ–è¡¨æƒ…è¯†åˆ«æœåŠ¡

        Args:
            model_path: ViTæ¨¡å‹è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„
        """
        # ä½¿ç”¨æœ¬åœ°æ¨¡å‹ç»å¯¹è·¯å¾„
        if model_path is None:
            # ä»å½“å‰æ–‡ä»¶ä½ç½®å‘ä¸ŠæŸ¥æ‰¾é¡¹ç›®æ ¹ç›®å½•
            current_file = os.path.abspath(__file__)
            current_dir = os.path.dirname(current_file)  # backend/app/services
            current_dir = os.path.dirname(current_dir)  # backend/app
            current_dir = os.path.dirname(current_dir)  # backend
            project_root = os.path.dirname(current_dir)  # é¡¹ç›®æ ¹ç›®å½•
            self.model_path = os.path.join(project_root, "facial_emotions_image_detection", "checkpoint-15740")
        else:
            self.model_path = model_path
        self.processor = None
        self.model = None
        self.face_cascade = None
        self.is_initialized = False

        # æƒ…ç»ªæ ‡ç­¾æ˜ å°„
        self.emotion_labels = {
            0: 'anger',
            1: 'disgust',
            2: 'fear',
            3: 'happy',
            4: 'neutral',
            5: 'sad',
            6: 'surprise'
        }

        # ä¸­æ–‡æƒ…ç»ªæ ‡ç­¾æ˜ å°„
        self.emotion_labels_chinese = {
            'anger': 'æ„¤æ€’',
            'disgust': 'åŒæ¶',
            'fear': 'ææƒ§',
            'happy': 'å¼€å¿ƒ',
            'neutral': 'å¹³é™',
            'sad': 'æ‚²ä¼¤',
            'surprise': 'æƒŠè®¶'
        }

    async def initialize(self) -> bool:
        """
        åˆå§‹åŒ–æ¨¡å‹å’Œæ£€æµ‹å™¨

        Returns:
            bool: åˆå§‹åŒ–æ˜¯å¦æˆåŠŸ
        """
        try:
            logger.info("ğŸš€ å¼€å§‹åˆå§‹åŒ–è¡¨æƒ…è¯†åˆ«æœåŠ¡...")

            # åˆå§‹åŒ–OpenCVé¢éƒ¨æ£€æµ‹å™¨
            logger.info("ğŸ“· åŠ è½½OpenCVé¢éƒ¨æ£€æµ‹å™¨...")
            # ä½¿ç”¨å†…ç½®çš„Haar Cascadeåˆ†ç±»å™¨
            cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
            self.face_cascade = cv2.CascadeClassifier(cascade_path)

            if self.face_cascade.empty():
                logger.error("âŒ OpenCVé¢éƒ¨æ£€æµ‹å™¨åŠ è½½å¤±è´¥")
                return False

            logger.info("âœ… OpenCVé¢éƒ¨æ£€æµ‹å™¨åŠ è½½æˆåŠŸ")

            # åˆå§‹åŒ–ViTæ¨¡å‹
            logger.info("ğŸ¤– åŠ è½½ViTè¡¨æƒ…è¯†åˆ«æ¨¡å‹...")
            self.processor = ViTImageProcessor.from_pretrained(self.model_path)
            self.model = ViTForImageClassification.from_pretrained(self.model_path)

            # æ–°æ¨¡å‹å·²æœ‰æ­£ç¡®çš„æ ‡ç­¾æ˜ å°„ï¼Œæ— éœ€æ‰‹åŠ¨è®¾ç½®
            logger.info(f"ğŸ“‹ æ¨¡å‹æ ‡ç­¾æ˜ å°„: {self.model.config.id2label}")

            # ç§»åŠ¨æ¨¡å‹åˆ°GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if torch.cuda.is_available():
                self.model = self.model.to('cuda')
                logger.info("ğŸš€ æ¨¡å‹å·²ç§»åŠ¨åˆ°GPU")
            else:
                logger.info("ğŸ’» ä½¿ç”¨CPUè¿›è¡Œæ¨ç†")

            self.is_initialized = True
            logger.info("âœ… è¡¨æƒ…è¯†åˆ«æœåŠ¡åˆå§‹åŒ–å®Œæˆ")

            return True

        except Exception as e:
            logger.error(f"âŒ è¡¨æƒ…è¯†åˆ«æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    def detect_faces(self, frame: np.ndarray) -> List[Tuple[int, int, int, int]]:
        """
        æ£€æµ‹å›¾åƒä¸­çš„é¢éƒ¨

        Args:
            frame: è¾“å…¥å›¾åƒå¸§ (BGRæ ¼å¼)

        Returns:
            List[Tuple[int, int, int, int]]: é¢éƒ¨è¾¹ç•Œæ¡†åˆ—è¡¨ (x, y, w, h)
        """
        if not self.is_initialized or self.face_cascade is None:
            logger.error("âŒ è¡¨æƒ…è¯†åˆ«æœåŠ¡æœªåˆå§‹åŒ–")
            return []

        try:
            # è½¬æ¢ä¸ºç°åº¦å›¾åƒ
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

            # æ£€æµ‹é¢éƒ¨
            faces = self.face_cascade.detectMultiScale(
                gray,
                scaleFactor=1.1,    # æœç´¢æ¯”ä¾‹
                minNeighbors=5,     # æœ€å°é‚»å±…æ•°
                minSize=(30, 30),   # æœ€å°é¢éƒ¨å¤§å°
                flags=cv2.CASCADE_SCALE_IMAGE
            )

            logger.info(f"ğŸ“Š æ£€æµ‹åˆ° {len(faces)} ä¸ªäººè„¸")

            # ç¡®ä¿ faces æ˜¯æ­£ç¡®çš„ç±»å‹å†è½¬æ¢
            if hasattr(faces, 'tolist'):
                return faces.tolist()
            elif isinstance(faces, (list, tuple)):
                return list(faces)
            else:
                logger.warning(f"âš ï¸ æ„å¤–çš„é¢éƒ¨æ£€æµ‹ç»“æœç±»å‹: {type(faces)}")
                return []

        except Exception as e:
            logger.error(f"âŒ é¢éƒ¨æ£€æµ‹å¤±è´¥: {e}")
            return []

    def preprocess_face(self, face_img: np.ndarray) -> Image.Image:
        """
        é¢„å¤„ç†é¢éƒ¨å›¾åƒä»¥é€‚é…ViTæ¨¡å‹

        Args:
            face_img: é¢éƒ¨å›¾åƒ (BGRæ ¼å¼)

        Returns:
            PIL.Image: é¢„å¤„ç†åçš„å›¾åƒ
        """
        try:
            # ç¡®ä¿å›¾åƒæ˜¯æ­£æ–¹å½¢
            height, width = face_img.shape[:2]
            if width != height:
                # è®¡ç®—æ­£æ–¹å½¢è¾¹ç•Œæ¡†
                size = min(width, height)
                x = (width - size) // 2
                y = (height - size) // 2
                face_img = face_img[y:y+size, x:x+size]

            # è°ƒæ•´å¤§å°ä¸º224x224 (ViTæ¨¡å‹è¾“å…¥)
            face_resized = cv2.resize(face_img, (224, 224))

            # è½¬æ¢ä¸ºRGBæ ¼å¼
            face_rgb = cv2.cvtColor(face_resized, cv2.COLOR_BGR2RGB)

            # è½¬æ¢ä¸ºPIL Image
            pil_image = Image.fromarray(face_rgb)

            return pil_image

        except Exception as e:
            logger.error(f"âŒ é¢éƒ¨å›¾åƒé¢„å¤„ç†å¤±è´¥: {e}")
            # è¿”å›ä¸€ä¸ªç©ºç™½å›¾åƒä½œä¸ºåå¤‡
            return Image.new('RGB', (224, 224), color='black')

    def predict_emotion(self, face_image: Image.Image) -> Dict:
        """
        ä½¿ç”¨ViTæ¨¡å‹é¢„æµ‹è¡¨æƒ…

        Args:
            face_image: é¢„å¤„ç†åçš„é¢éƒ¨å›¾åƒ

        Returns:
            Dict: åŒ…å«æƒ…ç»ªé¢„æµ‹ç»“æœçš„å­—å…¸
        """
        if not self.is_initialized or self.processor is None or self.model is None:
            logger.error("âŒ ViTæ¨¡å‹æœªåˆå§‹åŒ–")
            return self._get_default_result()

        try:
            # é¢„å¤„ç†å›¾åƒ
            inputs = self.processor(images=face_image, return_tensors="pt")

            # ç§»åŠ¨åˆ°GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if torch.cuda.is_available():
                inputs = {k: v.to('cuda') for k, v in inputs.items()}

            # æ¨¡å‹æ¨ç†
            with torch.no_grad():
                outputs = self.model(**inputs)

            # è·å–é¢„æµ‹ç»“æœ
            logits = outputs.logits
            predicted_class = logits.argmax(-1).item()
            probabilities = torch.softmax(logits, dim=1)[0]

            # æ„å»ºç»“æœ
            emotion_english = self.emotion_labels[predicted_class]
            emotion_chinese = self.emotion_labels_chinese[emotion_english]
            confidence = probabilities[predicted_class].item()

            probabilities_dict = {
                label: prob.item()
                for label, prob in zip(self.emotion_labels.values(), probabilities)
            }

            result = {
                "emotion": emotion_english,
                "emotion_chinese": emotion_chinese,
                "confidence": confidence,
                "probabilities": probabilities_dict,
                "timestamp": random.randint(1000000, 9999999)
            }

            logger.info(f"ğŸ˜Š è¡¨æƒ…è¯†åˆ«ç»“æœ: {emotion_chinese} ({confidence:.3f})")
            logger.info(f"ğŸ“Š è¯¦ç»†æ¦‚ç‡: {probabilities_dict}")
            return result

        except Exception as e:
            logger.error(f"âŒ è¡¨æƒ…é¢„æµ‹å¤±è´¥: {e}")
            return self._get_default_result()

    def _get_default_result(self) -> Dict:
        """
        è·å–é»˜è®¤çš„é¢„æµ‹ç»“æœï¼ˆç”¨äºé”™è¯¯æƒ…å†µï¼‰

        Returns:
            Dict: é»˜è®¤ç»“æœ
        """
        return {
            "emotion": "neutral",
            "emotion_chinese": "å¹³é™",
            "confidence": 0.5,
            "probabilities": {
                "anger": 0.1,
                "disgust": 0.1,
                "fear": 0.1,
                "happy": 0.2,
                "neutral": 0.3,
                "sad": 0.1,
                "surprise": 0.1
            },
            "timestamp": random.randint(1000000, 9999999)
        }

    async def process_frame(self, frame_data: bytes) -> Dict:
        """
        å¤„ç†è§†é¢‘å¸§å¹¶è¿”å›è¡¨æƒ…è¯†åˆ«ç»“æœ

        Args:
            frame_data: JPEGæ ¼å¼çš„å›¾åƒå­—èŠ‚æ•°æ®

        Returns:
            Dict: è¡¨æƒ…è¯†åˆ«ç»“æœ
        """
        try:
            logger.info(f"ğŸ“¹ å¼€å§‹å¤„ç†è§†é¢‘å¸§ï¼Œå¤§å°: {len(frame_data)} bytes")

            # è§£ç å›¾åƒ
            nparr = np.frombuffer(frame_data, np.uint8)
            frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)

            if frame is None:
                logger.error("âŒ å›¾åƒè§£ç å¤±è´¥")
                return self._get_default_result()

            logger.info(f"âœ… å›¾åƒè§£ç æˆåŠŸï¼Œå°ºå¯¸: {frame.shape}")

            # æ£€æµ‹é¢éƒ¨
            faces = self.detect_faces(frame)

            if len(faces) == 0:
                logger.info("ğŸ‘¤ æœªæ£€æµ‹åˆ°é¢éƒ¨")
                return {
                    "emotion": "neutral",
                    "emotion_chinese": "å¹³é™",
                    "confidence": 0.0,
                    "message": "æœªæ£€æµ‹åˆ°é¢éƒ¨",
                    "timestamp": random.randint(1000000, 9999999)
                }

            logger.info(f"ğŸ¯ æ£€æµ‹åˆ° {len(faces)} ä¸ªäººè„¸ï¼Œå¼€å§‹å¤„ç†ç¬¬ä¸€ä¸ª")

            # å¤„ç†ç¬¬ä¸€ä¸ªæ£€æµ‹åˆ°çš„äººè„¸
            x, y, w, h = faces[0]
            face_img = frame[y:y+h, x:x+w]

            logger.info(f"âœ‚ï¸ è£å‰ªé¢éƒ¨åŒºåŸŸ: x={x}, y={y}, w={w}, h={h}")

            # é¢„å¤„ç†é¢éƒ¨å›¾åƒ
            processed_face = self.preprocess_face(face_img)

            # é¢„æµ‹è¡¨æƒ…
            result = self.predict_emotion(processed_face)

            # æ·»åŠ é¢éƒ¨æ¡†ä¿¡æ¯
            result["face_box"] = {"x": int(x), "y": int(y), "width": int(w), "height": int(h)}
            result["faces_count"] = len(faces)

            logger.info(f"ğŸ‰ å¸§å¤„ç†å®Œæˆï¼Œè¿”å›ç»“æœ: {result.get('emotion_chinese', 'æœªçŸ¥')}")
            return result

        except Exception as e:
            logger.error(f"âŒ å¸§å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return self._get_default_result()

    def get_model_info(self) -> Dict:
        """
        è·å–æ¨¡å‹ä¿¡æ¯

        Returns:
            Dict: æ¨¡å‹ä¿¡æ¯
        """
        return {
            "model_name": "ViT-Large Facial Emotion Recognition (checkpoint-15740)",
            "model_path": self.model_path,
            "supported_emotions": list(self.emotion_labels_chinese.values()),
            "input_size": "224x224",
            "accuracy": "é¢„è®¡80%+",
            "is_initialized": self.is_initialized,
            "device": "cuda" if torch.cuda.is_available() else "cpu"
        }

# å…¨å±€æœåŠ¡å®ä¾‹
emotion_service = EmotionRecognitionService()
